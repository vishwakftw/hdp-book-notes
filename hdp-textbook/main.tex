\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{hyperref}
\usepackage{nicefrac}

\setlength{\parskip}{2mm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\newcommand{\real}{\mathbb{R}}
\newcommand{\Exp}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\inner}[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\indic}[1]{\mathbf{1}_{\{#1\}}}

\newcommand{\calN}{\mathcal{N}}

\title{Notes for High-Dimensional Probability}
\author{Vishwak Srinivasan}

\date{}

\begin{document}
\raggedright

\maketitle
\tableofcontents

\newpage

\section{Preliminaries}
\subsection{Example on approximate Caratheodory's Theorem}

First, we begin by discussing Caratheodory's Theorem:
\begin{theorem}[Caratheodory's Theorem]
\label{thm:approx-caratheodory}
Consider a convex set \(S \subseteq \real^{p}\). Any point \(x \in S\) can be represented as a convex combination of at most \(p + 1\) distinct points from \(S\).
\end{theorem}

\begin{remark}
This result is a popular result in convex analysis, and is tight. The tight lower bound is achieved by a simplex in \(p\) dimensions, which corresponds to \(p + 1\) vertices.
\end{remark}

Now, we seek an approximation of the above theorem like so: given \(k\) points \(\{x_{i}\}_{i=1}^{k} \subset S\), is it possible to approximate a point \(x \in S\)? We answer this in the affirmative below:
\begin{theorem}[Approx. Caratheodory's Theorem]
Given \(x \in S \subseteq \real^{p}\), where \(S\) is convex, there exists a set of \(k\) points \(\{x_{i}\}_{i=1}^{k} \in S\), such that the following holds:
\begin{equation*}
\left\|x - \frac{1}{k}\sum_{i=1}^{k}x_{i}\right\|_{2} \leq \frac{\mathrm{diam}(S)}{\sqrt{k}}
\end{equation*}
where \(\mathrm{diam}(S) = \sup\limits_{s, t \in S} \|s - t\|_{2}\).
\end{theorem}

\begin{proof}
By the fact that \(x \in S\), we know that we can write \(x\) as a convex combination of a subset \(\{z_{i}\}_{i=1}^{m}\) that satisfy \(\mathrm{CONV}(\{z_{i}\}_{i=1}^{m}) = S\), where \(m \leq p + 1\). Let the coefficients be \(\{\lambda_{i}\}_{i=1}^{m}\) where \(\sum\limits_{i=1}^{m} \lambda_{i} = 1\) and \(\lambda_{i} \geq 0\) for all \(i \in [m]\).

Consider a random variable \(Z\) that takes \(m\) different values from the set \(\{z_{i}\}_{i=1}^{m}\) with probability \(\lambda_{i}\). Note that \(\Exp[Z] = x\), since \(\Exp[Z] = \sum\limits_{i=1}^{m} \Pr(Z = z_{i})z_{i} = \sum\limits_{i=1}^{m} \lambda_{i}z_{i} = x\).

We know that for any \(x \in \real^{p}\) and independent random variables \(\{Z_{i}\}_{i=1}^{k}\) that satisfy \(\Exp[Z_{i}] = x\) for all \(i \in [k]\):
\begin{align*}
\Exp\left[\left\|x - \frac{1}{k}\sum_{i=1}^{k}Z_{i}\right\|_{2}^{2}\right] &= \Exp\left[\left\|\frac{1}{k}\sum_{i=1}^{k}(x - Z_{i})\right\|_{2}^{2}\right] \\
&= \frac{1}{k^{2}}\Exp\left[\left\|\sum_{i=1}^{k} (x - Z_{i})\right\|_{2}^{2}\right] \\
&\overset{(i)}= \frac{1}{k^{2}}\sum_{i=1}^{k}\Exp\left[\left\|x - Z_{i}\right\|_{2}^{2}\right] \\
&\overset{(ii)}\leq \frac{1}{k^{2}}\sum_{i=1}^{k}\mathrm{diam}(S)^{2} = \frac{\mathrm{diam}(S)^{2}}{k}
\end{align*}
Step \((i)\) holds true due to Lemma \ref{lem:zero-mean-norm}. Step \((ii)\) follows from the fact that \(Z_{i}, x \in S\) which implies that \(\|Z_{i} - x\|_{2} \leq \mathrm{diam}(S)\) followed by the fact that \(\Exp[c] = c\) for constant \(c\).

Therefore, there exists a realization of \(\{Z_{i}\}_{i=1}^{k}\), that satisfies:
\begin{equation*}
\left\|x - \frac{1}{k}\sum_{i=1}^{k}Z_{i}\right\|_{2} \leq \frac{\mathrm{diam}(S)}{\sqrt{k}}
\end{equation*}
\end{proof}

\begin{remark}
First note the dimension independence in the result. Secondly, in the special case where \(S\) consists of elements with bounded norms i.e., \(\|x\|_{2} \leq B\) for all \(x \in S\), the diameter of the set is bounded by \(2B\) by an application of the triangle inequality. Finally, note that if we have \(k \to \infty\) samples from the set, then our approximation is going to be perfect.
\end{remark}

The method used to prove Theorem \ref{thm:approx-caratheodory} is called Maurey's Empirical Method.

\subsubsection{Auxiliary Lemmata}

\begin{lemma}
\label{lem:zero-mean-norm}
Let \(\{X_{i}\}_{i=1}^{k}\) be a set of independent zero-mean random variables. The following holds true:
\begin{equation*}
\Exp\left[\left\|\sum_{i=1}^{k}X_{i}\right\|_{2}^{2}\right] = \sum_{i=1}^{k}\Exp\left[\left\|X_{i}\right\|_{2}^{2}\right]
\end{equation*}
\end{lemma}

\begin{proof}
First note that:
\begin{align*}
\left\|\sum_{i=1}^{k}X_{i}\right\|_{2}^{2} &= \inner{\sum_{i=1}^{k}X_{i}}{\sum_{j=1}^{k}X_{j}} \\
&= \sum_{i=1}^{k}\sum_{j=1}^{k}X_{i}^{T}X_{j} \\
&= \sum_{i=1}^{k}\|X_{i}\|_{2}^{2} + 2\sum_{\substack{i, j = 1 \\ i \neq j}}^{k}X_{i}^{T}X_{j}
\end{align*}

Taking expectations on both sides:
\begin{align*}
\Exp\left[\left\|\sum_{i=1}^{k}X_{i}\right\|_{2}^{2}\right] &= \Exp\left[\sum_{i=1}^{k}\|X_{i}\|_{2}^{2}\right] + 2\Exp\left[\sum_{\substack{i, j = 1 \\ i \neq j}}^{k}X_{i}^{T}X_{j}\right] \\
&= \sum_{i=1}^{k}\Exp\left[\left\|X_{i}\right\|_{2}^{2}\right] + 2\sum_{\substack{i, j = 1 \\ i \neq j}}^{k} \Exp\left[X_{i}^{T}X_{j}\right]
\end{align*}

Since \(X_{i}\)s are independent, \(\Exp\left[X_{i}^{T}X_{j}\right] = \Exp\left[X_{i}\right]^{T}\Exp\left[X_{j}\right] = 0\), and this completes the proof.
\end{proof}

\begin{lemma}
For all integers \(m \in [1, n]\), we have the following series of inequalities:
\begin{equation*}
\left(\frac{n}{m}\right)^{m} \leq \binom{n}{m} \leq \sum_{k=0}^{m}\binom{n}{m} \leq \left(\frac{en}{m}\right)^{m}
\end{equation*}
\end{lemma}

\begin{proof}
First inequality:
\begin{equation*}
\binom{n}{m} m^{m} = \frac{n!}{(n - m)! \cdot m!} m^{m} \geq \frac{n!}{(n - m)!} \geq n^{m} \Rightarrow \binom{n}{m} \geq \left(\frac{n}{m}\right)^{m}
\end{equation*}

Second inequality:
\begin{equation*}
\binom{n}{m} \leq \binom{n}{m} + \sum_{k=0}^{m-1} \binom{n}{k} = \sum_{k=0}^{m}\binom{n}{k}
\end{equation*}

Third inequality:
\begin{equation*}
\left(\frac{m}{n}\right)^{m} \sum_{k=0}^{m}\binom{n}{k} \leq \sum_{k=0}^{m} \binom{n}{k} \left(\frac{m}{n}\right)^{k} \leq \sum_{k=0}^{n} \binom{n}{k} \left(\frac{m}{n}\right)^{k} = \left(1 + \frac{m}{n}\right)^{n} \leq e^{m} \Rightarrow \sum_{k=0}^{m}\binom{n}{k} \leq \left(\frac{en}{m}\right)^{m}
\end{equation*}
\end{proof}

\subsection{Quantities and Inequalities associated with RVs}

\begin{itemize}
\item Expectation: \(\Exp[X]\)
\item Variance: \(\Var[X] = \Exp[(X - \Exp[X])^{2}]\)
\item MGF: \(M_{X}(t) = \Exp[e^{tX}]\), \(t \in \real\)
\item \(p^{th}\) moment: \(\Exp[X^{p}]\) and \(p^{th}\) absolute moment: \(\Exp[|X|^{p}]\)
\item \(L^{p}\) norm: \(\|X\|_{L^{p}} = \sqrt[p]{\Exp[|X|^{p}]}\)
\item \(L^{\infty}\) norm: \(\|X\|_{L^{\infty}} = \mathrm{ess} \sup |X|\), where \(\mathrm{ess} \sup |X|\) denotes the supremum over all set with measure not 0. Also note that: \(\mathrm{ess} \sup |X| \leq \sup |X|\).
\item Covariance: \(\Cov(X, Y) = \Exp[(X - \Exp[X])(Y - \Exp[Y])]\)
\item CDF: \(F_{X}(t) = \Pr(X \leq t)\), \(t \in \real\)
\end{itemize}

For a convex function \(f\) and any random variable \(X\), we have by \emph{Jensen's inequality} that:
\begin{equation*}
f(\Exp[X]) \leq \Exp[f(X)]
\end{equation*}

Consequently, for a concave function \(f\) and any random variable \(X\), we have:
\begin{equation*}
f(\Exp[X]) \geq \Exp[f(X)]
\end{equation*}

As a special case, consider \(f(x) : x^{\nicefrac{q}{p}}\) where \(q > p\). Note that \(f\) is convex. Therefore:
\begin{equation*}
(\Exp\left[|X|^{p}\right])^{\nicefrac{q}{p}} \leq \Exp\left[|X|^{q}\right] \Rightarrow \|X\|_{L^{p}} \leq \|X\|_{L^{q}}
\end{equation*}

Another inequality is the \emph{Cauchy-Schwarz inequality}, which states that for any two RVs \(X\) and \(Y\):
\begin{equation*}
\Exp[|XY|] \leq \sqrt{\Exp[X^{2}]}\sqrt{\Exp[Y^{2}]} = ||X||_{L^{2}} ||Y||_{L^{2}}
\end{equation*}

We also have \emph{Holder's inequality} which generalizes \emph{Cauchy-Schwarz} to dual norms as:
\begin{equation*}
\Exp[|XY|] \leq ||X||_{L^{p}} ||Y||_{L^{q}} \qquad;\qquad \frac{1}{p} + \frac{1}{q} = 1
\end{equation*}

The following lemma characterizes the expectation as a quantity involving only tails:
\begin{lemma}
\label{lem:tail-expectation}
Consider a non-negative random variable \(X\). The expectation of this random variable can be written as:
\begin{equation*}
\Exp[X] = \int_{0}^{\infty} \Pr(X > t) dt
\end{equation*}
\end{lemma}

\begin{proof}
For any \(x \geq 0\), we have that:
\begin{equation*}
x = \int_{0}^{\infty} \indic{t < x} dt = \int_{0}^{x} 1 dt + \int_{x}^{\infty} 0 dt
\end{equation*}

Therefore:
\begin{align*}
X = \int_{0}^{\infty} \indic{t < X} dt \Rightarrow \Exp[X] &= \Exp\left[\int_{0}^{\infty} \indic{t < X} dt\right] \\
& = \int_{0}^{\infty} \int_{-\infty}^{\infty} \indic{t < x} \Pr(X = x) dx dt \\
& = \int_{0}^{\infty} \int_{t}^{\infty} \Pr(X = x) dx dt \\
& = \int_{0}^{\infty} \Pr(X > t) dt
\end{align*}
\end{proof}

A simple generalization for real-valued random variables from the proof of Lemma \ref{lem:tail-expectation} is as follows:
\begin{corollary}
Consider a real valued random variable \(X\). The expectation of this random variable can be written as:
\begin{equation*}
\Exp[X] = \int_{0}^{\infty} \Pr(X > t) dt - \int_{-\infty}^{0} \Pr(X < t) dt
\end{equation*}
\end{corollary}

An application of Lemma \ref{lem:tail-expectation} is to use it to bound the \(p^{th}\) absolute moments via tails:
\begin{corollary}
For any random variable \(X\):
\begin{equation*}
\Exp\left[|X|^{p}\right] = \int_{0}^{\infty} pt^{p-1} \Pr(|X| > t) dt
\end{equation*}
\end{corollary}

Classical inequalities: Markov and Chebyshev's:
\begin{lemma}[Markov's Inequality]
Consider a non-negative random variable \(X\). Then the tails of \(X\) can be bounded as:
\begin{equation*}
\Pr(X > t) \leq \frac{\Exp[X]}{t}
\end{equation*}
\end{lemma}

\begin{proof}
Note that:
\begin{equation*}
\Exp[X] = \Exp[X \cdot \indic{X > t}] + \Exp[X \cdot \indic{X \leq t}] \geq \Exp[X \cdot \indic{X > t}] \geq t \Exp[\indic{X > t}] = t \Pr(X > t) \Rightarrow \Pr(X > t) \leq \frac{\Exp[X]}{t}
\end{equation*}
\end{proof}

\begin{corollary}[Chebyshev's Inequality]
Consider a random variable \(X\). Then the probability of deviation from the expectation of \(X\) can be bounded as:
\begin{equation*}
\Pr(|X - \Exp[X]| > t) \leq \frac{\Var(X)}{t^{2}}
\end{equation*}
\end{corollary}

\begin{proof}
Take \(Y = |X - \Exp[X]|\) as the random variable as apply Markov's inequality:
\begin{equation*}
\Pr(Y > t) = \Pr(Y^{2} > t^{2}) \leq \frac{\Exp[(X - \Exp[X])^{2}]}{t^{2}}
\end{equation*}
\end{proof}

\begin{remark}
Note that one can achieve better dependence on \(t\) by using higher moments - provided they exist:
\begin{equation*}
\Pr(Y > t) = \Pr(Y^{2k} > t^{2k}) \leq \frac{\Exp[(X - \Exp[X])^{2k}]}{t^{2k}}
\end{equation*}
\end{remark}

\subsection{Basic Limit Theorems}
\begin{theorem}[Strong Law of Large Numbers]
Let \(\{X_{i}\}_{i=1}^{n}\) be a sequence of identically and independently distributed random variables with mean \(\mu\). The quantity \(\bar{X}_{n} = \frac{1}{n}\sum\limits_{i=1}^{n} X_{i}\) satisfies:
\begin{equation*}
\bar{X}_{n} \xrightarrow{a.s.} \mu
\end{equation*}
as \(n \to \infty\).
\end{theorem}

Here \(\xrightarrow{a.s.}\) denotes \emph{almost sure convergence}, which is:
\begin{equation*}
\Pr\left(\lim_{n \to \infty} \bar{X}_{n} = \mu\right) = 1
\end{equation*}

There is a \emph{weak law of large numbers}, which can be derived from Chebyshev's Inequality, for distributions with bounded variance. It is stated below:
\begin{corollary}[Weak Law of Large Numbers]
Let \(\{X_{i}\}_{i=1}^{n}\) be a sequence of identically and independently distributed random variables with mean \(\mu\) and variance \(\sigma^{2} < \infty\). The quantity \(\bar{X}_{n} = \frac{1}{n}\sum\limits_{i=1}^{n} X_{i}\) satisfies:
\begin{equation*}
\bar{X}_{n} \xrightarrow{p} \mu
\end{equation*}
where \(\xrightarrow{p}\) denotes \emph{convergence in probability}, which is;
\begin{equation*}
\forall \epsilon > 0, \qquad \lim_{n \to \infty} \Pr\left(|\bar{X}_{n} - \mu| > \epsilon\right) = 0
\end{equation*}
\end{corollary}

\begin{proof}
First note \(\Exp\left[\bar{X}_{n}\right] = \mu\), and hence \(\Var(\bar{X}_{n}) = \frac{1}{n^{2}}\Var\left(\sum\limits_{i=1}^{n}X_{i}\right) = \frac{1}{n^{2}}\sum\limits_{i=1}^{n}\Var\left(X_{i}\right) = \frac{\sigma^{2}}{n}\).

By Chebyshev's inequality, for any \(\epsilon > 0\):
\begin{equation*}
\Pr(|\bar{X}_{n} - \mu| > \epsilon) \leq \frac{\sigma^{2}}{n\epsilon} \Rightarrow \lim_{n \to \infty} \Pr(|\bar{X}_{n} - \mu| > \epsilon) = 0 \enskip (\because \text{Sandwich theorem})
\end{equation*}
\end{proof}

\begin{remark}
This weak result is \emph{weak} because \(\xrightarrow{a.s.}\) implies \(\xrightarrow{p.}\).
\end{remark}

Next, we state a result that gives the asymptotic distribution of \(\bar{X}_{n}\).
\begin{theorem}[Central Limit Theorem]
\label{thm:clt}
Let \(\{X_{i}\}_{i=1}^{n}\) be a sequence of identically and independently distributed random variables with mean \(\mu\) and variance \(\sigma^{2} < \infty\). Define \(\bar{X}_{n} = \frac{1}{n}\sum\limits_{i=1}^{n} X_{i}\). Then:
\begin{equation*}
\frac{\sqrt{n}(\bar{X}_{n} - \mu)}{\sigma} \xrightarrow{d} \calN(0, 1) \qquad \text{as } n \to \infty
\end{equation*}
\end{theorem}

While this result states that the deviation between the sample mean and population mean is 0 in the limit, we can give some non asymptotic guarantees on the deviation as follows:
\begin{lemma}
Let \(\{X_{i}\}_{i=1}^{n}\) be a sequence of identically and independently distributed random variables with mean \(\mu\) and variance \(\sigma^{2} < \infty\). We have that:
\begin{equation*}
\Exp\left[\left|\frac{1}{n}\sum_{i=1}^{n}X_{i} - \mu\right|\right] = O\left(\frac{1}{\sqrt{n}}\right)
\end{equation*}
\end{lemma}

\begin{proof}
By Jensen's inequality:
\begin{equation*}
\Exp\left[|Z|\right] \leq \sqrt{\Exp\left[Z^{2}\right]}
\end{equation*}
(Note that this also follows from the fact that \(\|Z\|_{L^{1}} \leq \|Z\|_{L^{2}}\))

Therefore:
\begin{align*}
\Exp\left[\left|\frac{1}{n}\sum_{i=1}^{n}X_{i} - \mu\right|\right] &\leq \sqrt{\Exp\left[\left(\frac{1}{n}\sum_{i=1}^{n}X_{i} - \mu\right)^{2}\right]} \\
&= \sqrt{\Exp\left[\left(\frac{1}{n}\sum_{i=1}^{n}(X_{i} - \mu)\right)^{2}\right]} \\
&= \sqrt{\frac{1}{n^{2}}\Exp\left[\left(\sum_{i=1}^{n}(X_{i} - \mu)\right)^{2}\right]} \\
&\overset{(i)}= \sqrt{\frac{1}{n^{2}}\sum_{i=1}^{n}\Exp\left[(X_{i} - \mu)^{2}\right]} \\
&= \sqrt{\frac{\sigma^{2}}{n}} = O\left(\frac{1}{\sqrt{n}}\right)
\end{align*}
where Step \((i)\) follows from Lemma \ref{lem:zero-mean-norm} for 1-D random variables.
\end{proof}

A special case of the Central Limit Theorem is to provide approximate distributions for binomial distributions. Recall that the binomial distribution \(\mathrm{Bin}(n, p)\) is the sum of \(n\) independent Bernoulli distribution with parameter \(p\). Therefore, we get that:
\begin{equation*}
\frac{\sqrt{n}(\bar{X}_{n} - \mu)}{\sigma} = \frac{n\bar{X}_{n} - n\mu}{\sigma\sqrt{n}} = \frac{B_{n, p} - np}{\sqrt{n}\sqrt{p(1 - p)}} \xrightarrow{d} \calN(0, 1) \enskip \text{as } n \to \infty
\end{equation*}
where \(X_{i} \sim \mathrm{Ber}(p), i \in [n]\) and \(B_{n, p} \sim \mathrm{Bin}(n, p)\). This means that \(B_{n, p} \xrightarrow{d} \calN(np, np(1 - p))\) as \(n \to \infty\).

However, there is a better limit theorem in the regime where \(p \to \infty, n \to \infty\) and \(np = \lambda > 0\). This is the Poisson Limit Theorem:
\begin{theorem}[Poisson Limit Theorem]
Consider \(\{X_{i}\}_{i=1}^{n}\) to be \(n\) independent Bernoulli variables with parameters \(p_{i}\). Then, for \(n \to \infty\), \(\max\limits_{i \in [n]} p_{i} \to 0\) and \(\sum\limits_{i=1}^{n}p_{i} = \lambda > 0\), we have that:
\begin{equation*}
\sum_{i=1}^{n}X_{i} \xrightarrow{d} \mathrm{Poi}(\lambda)
\end{equation*}
\end{theorem}

\begin{remark}
In the special case when all \(p_{i}\)s are equal, we obtain the same result with \(n \to \infty\), \(p \to 0\) and \(np = \lambda > 0\) as described informally earlier.
\end{remark}

\newpage

\section{Concentration inequalities}
\subsection{Basic Gaussian Inequalities}
\begin{lemma}[Mill's inequalities]
Let \(g \sim \calN(0, 1)\). We have the following lower and upper bounds for the tail \(\Pr(g > t)\), \(t > 0\) as follows:
\begin{equation*}
\left(\frac{1}{t} - \frac{1}{t^{3}}\right)\frac{1}{\sqrt{2\pi}}e^{-\nicefrac{t^{2}}{2}} \leq \Pr(g > t) \leq \frac{1}{t}\cdot\frac{1}{\sqrt{2\pi}}e^{-\nicefrac{t^{2}}{2}}
\end{equation*}
\end{lemma}

\begin{proof}
First, the upper bound:
\begin{align*}
\Pr(g > t) &= \frac{1}{\sqrt{2\pi}}\int_{t}^{\infty} e^{-\nicefrac{x^{2}}{2}} dx \\
&= \frac{1}{t} \cdot \frac{1}{\sqrt{2\pi}}\int_{t}^{\infty} te^{-\nicefrac{x^{2}}{2}} dx \\
&\leq \frac{1}{t} \cdot \frac{1}{\sqrt{2\pi}}\int_{t}^{\infty} xe^{-\nicefrac{x^{2}}{2}} dx \\
&= \frac{1}{t} \cdot \frac{1}{\sqrt{2\pi}}\int_{\nicefrac{t^{2}}{2}}^{\infty} ye^{-y} dy \qquad \left(\because y = \frac{x^{2}}{2}\right)\\
&= \frac{1}{t} \cdot \frac{1}{\sqrt{2\pi}}e^{-\nicefrac{t^{2}}{2}}
\end{align*}

Second, the lower bound:
\begin{align*}
\Pr(g > t) &= \frac{1}{\sqrt{2\pi}}\int_{t}^{\infty} e^{-\nicefrac{x^{2}}{2}} dx \\
&\geq \frac{1}{\sqrt{2\pi}}\int_{t}^{\infty} \left(1 - \frac{3}{x^{4}}\right)e^{-\nicefrac{x^{2}}{2}} dx \qquad \left(\because 1 - \frac{3}{x^{4}} \leq 1 \enskip \forall \enskip x > 0\right) \\
&\geq \left(\frac{1}{t} - \frac{1}{t^{3}}\right)\frac{1}{\sqrt{2\pi}}e^{-\nicefrac{t^{2}}{2}}
\end{align*}
\end{proof}

\begin{remark}
Note that one can get tail bounds for \(\calN(0, \sigma^{2})\) by simply reparameterising the integrals as:
\begin{equation*}
\left(\frac{\sigma}{t} - \frac{\sigma}{t^{3}}\right)\frac{1}{\sqrt{2\pi}}e^{-\nicefrac{t^{2}}{2}} \leq \Pr(g > t) \leq \frac{\sigma}{t}\cdot\frac{1}{\sqrt{2\pi}}e^{-\nicefrac{t^{2}}{2}}
\end{equation*}
\end{remark}

The Central Limit Theorem (Theorem \ref{thm:clt}) states that averages tend in distribution to a Gaussian. But what can be said about the distribution function itself? The following theorem gives this result:
\begin{theorem}[Berry-Esseen CLT]
Let \(\{X_{i}\}_{i=1}^{n}\) be a sequence of identically and independently distributed random variables with mean \(\mu\) and variance \(\sigma^{2} < \infty\). Define \(\bar{X}_{n} = \frac{1}{n}\sum\limits_{i=1}^{n} X_{i}\) and \(\bar{Z}_{n} = \sqrt{n}\frac{\bar{X}_{n} - \mu}{\sigma}\). Then:
\begin{equation*}
\left|\Pr(\bar{Z}_{n} > t) - \Pr(g > t))\right| \leq \frac{\rho}{\sqrt{n}}
\end{equation*}
where \(\rho = \frac{\Exp\left[|X_{i} - \mu|^{3}\right]}{\sigma^{3}}\), \(i \in [n]\) and \(g \sim \calN(0, 1)\).
\end{theorem}

\begin{remark}
This theorem basically states that the error of approximation scales as \(O\left(\frac{1}{\sqrt{n}}\right)\), which is bad, since we can't always leverage the normal approximation from the central limit theorem always.
\end{remark}

\subsubsection{Auxiliary Lemmata}
\begin{lemma}
Let \(g \sim \calN(0, 1)\). For \(t \geq 1\), we have that:
\begin{equation*}
\Exp\left[g^{2} \indic{g > t}\right] = \frac{t}{\sqrt{2\pi}}e^{-\nicefrac{t^{2}}{2}} + \Pr(g > t) \leq \left(t + \frac{1}{t}\right)\frac{1}{\sqrt{2\pi}}e^{-\nicefrac{t^{2}}{2}}
\end{equation*}
\end{lemma}

\begin{proof}
\begin{align*}
\Exp\left[g^{2} \indic{g > t}\right] &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} x^{2} \indic{x > t} e^{-\nicefrac{x^{2}}{2}} dx \\
&= \frac{1}{\sqrt{2\pi}}\int_{t}^{\infty} x^{2} e^{-\nicefrac{x^{2}}{2}} dx \\
&= \frac{1}{\sqrt{2\pi}}\left(\left.\left(x \cdot e^{-x}\right)\right\rvert_{\nicefrac{t^{2}}{2}}^{\infty} + \int_{t}^{\infty} e^{-\nicefrac{x^{2}}{2}}dx\right) \qquad \left(\because \text{int. by parts with } f(x) = x, g(x) = xe^{-\nicefrac{x^{2}}{2}}\right) \\
&= \frac{t}{\sqrt{2\pi}}e^{-\nicefrac{t^{2}}{2}} + \Pr(g > t) \\
&\leq \frac{t}{\sqrt{2\pi}}e^{-\nicefrac{t^{2}}{2}} + \frac{1}{t}\cdot \frac{1}{\sqrt{2\pi}}e^{-\nicefrac{t^{2}}{2}} \\
&= \left(t + \frac{1}{t}\right)\frac{1}{\sqrt{2\pi}}e^{-\nicefrac{t^{2}}{2}}
\end{align*}
\end{proof}

\end{document}
