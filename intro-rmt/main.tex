\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{hyperref}
\usepackage{nicefrac}
\usepackage{xcolor}

\setlength{\parskip}{2mm}

\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{corollary}{Corollary}[subsection]
\newtheorem{lemma}{Lemma}[subsection]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\newcommand{\comprule}{\textcolor[RGB]{220,220,220}{\rule{\linewidth}{0.2pt}}}

\newcommand{\real}{\mathbb{R}}
\newcommand{\Exp}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\inner}[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\indic}[1]{\mathbf{1}_{\{#1\}}}

\newcommand{\calN}{\mathcal{N}}
\newcommand{\calE}{\mathcal{E}}

\title{Notes for Random Matrix Theory}
\author{Vishwak Srinivasan}

\date{}

\begin{document}
\raggedright

\maketitle
\tableofcontents

\newpage

\section{Introduction}
A useful fact is the following: \emph{tall matrices are approximate isometries}. Let's parse this statement.
\begin{itemize}
\item Tall matrices are those matrices \(A \in \real^{N \times n}\) where \(N \gg n\).
\item Approximate isometries: consider the vector space \(\real^{n}\) and \(\real^{N}\). A tall matrix transforms a vector \(x \in \real^{n}\) to \(Ax \in \real^{N}\). Mathematically:
\begin{equation*}
(1 - \delta)K\|x\|_{2} \leq \|Ax\|_{2} \leq (1 + \delta)K\|x\|_{2}
\end{equation*}
where \(K\) is a normalization factor and \(\delta \ll 1\).
This looks like a version of Johnson-Lindenstrauss.
\end{itemize}

Now divide by \(\|x\|_{2}\) to get:
\begin{equation*}
(1 - \delta)K \leq \frac{\|Ax\|_{2}}{\|x\|_{2}} \leq (1 + \delta)K \Rightarrow (1 - \delta)K \leq \sigma_{\min}(A) \leq \sigma_{\max}(A) \leq (1 + \delta)K
\end{equation*}
and this tells us that the range of singular values is small. Furthermore, the condition number be bounded as: \(\kappa(A) \leq \frac{1 + \delta}{1 - \delta} \approx 1\) for the specified bound on \(\delta\). Therefore, tall matrices are well conditioned \emph{always}.

\subsection{Some miscellaneous results}
\(\epsilon\)-nets are a neat way of computing quantities that can be expressed over balls.
An \(\epsilon\)-net of a set \(S\) is a set of points \(\calN_{\epsilon}(S)\) that approximates a point in the original set.
That is to say, for every \(x \in S\), there exists \(y \in \calN_{\epsilon}(S)\) such that \(\|y - x\| \leq \epsilon\).
As you can see, it is define w.r.t a norm. The cardinality of the \(\calN_{\epsilon}(S)\) is called the \(\epsilon\)-covering number of \(S\) (w.r.t. a norm).

Now, let's look at an application of nets in computing the \(\ell_{2}\)-norm of a vector.
\begin{lemma}
Let \(x \in \real^{p}\). Then:
\begin{equation*}
\|x\|_{2} \leq \frac{1}{1 - \epsilon}\sup_{v \in \calN_{\epsilon}(S^{p-1})} |v^{T}x|
\end{equation*}
\end{lemma}

\begin{proof}
Let \(v\) be the unit vector that results in \(|v^{T}x| = \|x\|_{2}\) (this is just unit vector in the direction of \(x\)). Now, let \(w \in \calN_{\epsilon}(S^{p-1})\) be the closest element to \(v\). By Cauchy-Schwarz:
\begin{equation*}
|(v - w)^{T}x| \leq \|v - w\|_{2}\|x\|_{2} \leq \epsilon \|x\|_{2}
\end{equation*}

Hence, by triangle inequality:
\begin{equation*}
|w^{T}x| \geq |v^{T}x| - |(w - v)^{T}x| \geq \|x\|_{2} - \epsilon\|x\|_{2} \Rightarrow \|x\|_{2} \leq \frac{1}{1 - \epsilon}|w^{T}x| \leq \frac{1}{1 - \epsilon} \sup_{w \in \calN_{\epsilon}(S^{p-1})}|w^{T}x|
\end{equation*}
\end{proof}

\begin{remark}
An important takeaway is that an optimization problem over an uncountable set, has now be reduced to a countable set, at the cost of some sub-optimality. The \(\epsilon\)-covering number of \(S^{p-1}\) is \(\left(1 + \frac{2}{\epsilon}\right)^{p}\).
\end{remark}

Back to random matrix theory, let's use an idea from the above lemma to compute the spectral norm of a matrix \(A\).
\begin{lemma}
Let \(A \in \real^{N \times n}\). Then:
\begin{equation*}
\|A\|_{2} \leq \frac{1}{1 - \epsilon}\sup_{v \in \calN_{\epsilon}(S^{n-1})} \|Av\|_{2}
\end{equation*}
\end{lemma}

\begin{proof}
The proof is the same from earlier. Let \(v\) be unit vector leading to \(\|Av\|_{2} = \|A\|_{2}\). Choose \(w \in \calN_{\epsilon}(S^{n-1})\) closest to \(v\). Using the fact that \(\|Ax\|_{2} \leq \|A\|_{2}\|x\|_{2}\):
\begin{equation*}
\|A(w - v)\| \leq \|A\|_{2}\|w - v\|_{2} \leq \|A\|_{2}\epsilon
\end{equation*}

By triangle inequality:
\begin{equation*}
\|Aw\|_{2} \geq \|Av\|_{2} - \|A(w - v)\|_{2} \geq \|A\|_{2} - \epsilon\|A\|_{2} \Rightarrow \|Aw\|_{2} \leq \frac{1}{1 - \epsilon}\|A\|_{2} \leq \frac{1}{1 - \epsilon} \sup_{w \in \calN_{\epsilon}(S^{n-1})}\|Aw\|_{2}
\end{equation*}
\end{proof}

\section{Random matrices}
\subsection{Independent entries}
We will consider the following setup: entries of the matrices being analyzed are independent and zero mean.

Let's look at a asymptotic result, famously known as the Bai-Yin's law:
\begin{theorem}[Bai-Yin's law]
Let \(A\) be an \(N \times n\) matrix whose entries are independent copies of a random variable with zero mean, unit variance and finite \(4^{th}\) moment. Consider the scenario where \(N, n \to \infty\) while \(\frac{n}{N} = c \in [0, 1]\). Then:
\begin{equation*}
\sigma_{\min}(A) = \sqrt{N} - \sqrt{n} + o(\sqrt{n}) \qquad \sigma_{\max}(A) = \sqrt{N} + \sqrt{n} + o(\sqrt{n})
\end{equation*}
almost surely.
\end{theorem}

To revisit notation, \(f(n) \in o(\sqrt{n})\) is such that \(\lim\limits_{n\to\infty} \frac{f(n)}{\sqrt{n}} = 0\). A non-asymptotic version of the Bai-Yin's law is as follows:
\begin{theorem}[Gordon's Theorem]
Let \(A\) be an \(N \times n\) matrix whose entries are independent standard normal variables. Then:
\begin{equation*}
\sqrt{N} - \sqrt{n} \leq \Exp[\sigma_{\min}(A)] \leq \Exp[\sigma_{\max}(A)] \leq \sqrt{N} + \sqrt{n}
\end{equation*}
\end{theorem}

\begin{proof}
For our proof, we will use the following theorem due to Sudakov and Fernique.
\begin{theorem}[Sudakov-Fernique Theorem]
Let \((X_{t})_{t \in T}\) and \((Y_{t})_{t \in T}\) be two Gaussian processes satisfying:
\begin{equation*}
\Exp\left[(X_{s} - X_{t})^{2}\right] \leq \Exp\left[(Y_{s} - Y_{t})^{2}\right]
\end{equation*}
for all \(s, t \in T\) and \(T\) being an abstract set. Then:
\begin{equation*}
\Exp\left[\sup_{t \in T} X_{t}\right] \leq \Exp\left[\sup_{t \in T}Y_{t}\right]
\end{equation*}
\end{theorem}

First note that:
\begin{equation*}
\sigma_{\max}(A) = \sup_{u \in S^{n-1}} \sup_{v \in S^{N - 1}} \inner{Au}{v}
\end{equation*}

Therefore, for a Gaussian process defined as \(X_{u, v} = \inner{Au}{v}\), we have that:
\begin{equation*}
\Exp\left[\sup_{t \in T}X_{u, v}\right] = \Exp[\sigma_{\max}(A)]
\end{equation*}
for \(T = S^{N-1} \times S^{n-1}\). To complement this, we define another Gaussian process \(Y_{u, v} = \inner{g}{u} + \inner{h}{v}\), where \(g\) and \(h\) are Gaussian random vectors. For this Gaussian process, we have:
\begin{equation*}
\Exp\left[\sup_{t \in T} Y_{u, v}\right] = \Exp\left[\|g\|_{2} + \|h\|_{2}\right] = \sqrt{N} + \sqrt{n}
\end{equation*}

It remains to check if \(\Exp\left[(X_{u, v} - X_{u',v'})^{2}\right] \leq \Exp\left[(Y_{u, v} - Y_{u', v'})^{2}\right]\).
\begin{equation*}
\Exp\left[(X_{u, v} - X_{u', v'})^{2}\right] \overset{(i)}= \sum_{i=1}^{N}\sum_{j=1}^{n}(v_{i}u_{j} - v'_{i}u'_{j})^{2} \overset{(ii)}\leq \|u - u'\|_{2}^{2} + \|v - v'\|_{2}^{2} \overset{(iii)}= \Exp\left[(Y_{u, v} - Y_{u', v'})^{2}\right]
\end{equation*}
Step \((i)\) uses Lemma \ref{lem:quadratic-random-mat-iid-entry}, Step \((ii)\) uses Lemma \ref{lem:inner-to-norm} and Step \((iii)\) uses Lemma \ref{lem:linear-random-vec}, and therefore the upper bound is complete.

For \(\sigma_{\min}(A)\), the respective program is:
\begin{equation*}
\sigma_{\min}(A) = \inf_{u \in S^{n-1}} \sup_{v \in S^{N - 1}} \inner{Au}{v}
\end{equation*}

There exists a result that generalizes the Sudakov-Fernique theorem to min-max objectives, and consequently results in the lower bound.
\end{proof}

\begin{remark}
\(X_{u, v}\) a Gaussian process because: \(\inner{Au}{v} = \sum\limits_{i=1}^{N}\sum\limits_{j=1}^{n}v_{i}u_{j}A_{ij} \sim \calN\left(0, \|v\|_{2}^{2}\|u\|_{2}^{2}\right)\). Similarly, \(Y_{u,v} = \inner{g}{u} + \inner{h}{v}\) is a Gaussian process because \(\inner{g}{u} \sim \calN(0, \|g\|_{2}^{2})\) and \(\inner{h}{v} \sim \calN(0, \|h\|_{2}^{2})\).
\end{remark}

Gordon's Theorem gives you bounds on the expected singular value. Via a Lipschitz concentration lemma, we can provide high probability estimates for instantiations of such matrices. The Lipschitz concentration lemma is stated below:
\begin{lemma}
\(X\) is a standard normal variable. Let \(f\) be an \(L\)-Lipschitz function i.e., \(|f(x) - f(y)| \leq L\|x - y\|_{2}\) for all \(x, y \in \real^{n}\). Then:
\begin{equation*}
\Pr\left(\left|f(X) - \Exp\left[f(X)\right]\right| > t\right) \leq 2\exp\left(-\frac{t^{2}}{2L^{2}}\right)
\end{equation*}
\end{lemma}

This is useful because \(\sigma_{\min}(A)\) and \(\sigma_{\max}(A)\) are \(1\)-Lipschitz functions of the inputs. The corollary below couples the results from above:
\begin{corollary}
Let \(A\) be an \(N \times n\) matrix with standard normal entries. Then with probability at least \(1 - 2\exp(\nicefrac{t^{2}}{2})\), we have that:
\begin{gather*}
\sqrt{N} - \sqrt{n} - t \leq \sigma_{\min}(A) \leq \sqrt{N} - \sqrt{n} + t \\
\sqrt{N} + \sqrt{n} + t \leq \sigma_{\max}(A) \leq \sqrt{N} + \sqrt{n} - t
\end{gather*}
\end{corollary}

We now go back to our older discussion on approximate isometries. A matrix \(\bar{A} = \frac{A}{\sqrt{N}}\) is an approximate isometry iff \(\bar{A}^{T}\bar{A}\) is an approximate identity. Why is this true? Recall from earlier:
\begin{equation*}
(1 - \delta)\|x\|_{2}^{2} \leq x^{T}\bar{A}^{T}\bar{A}x \leq (1 + \delta)\|x\|_{2}^{2} \Rightarrow |x^{T}(\bar{A}^{T}\bar{A} - I)x| \leq \delta \|x\|_{2}^{2}
\end{equation*}
where we have used \(K = N\). This might seem as a fragile explanation, however, but motivates future discussions about relating \(\bar{A}\) and \(\bar{A}^{T}\bar{A}\) in our analyses.

\begin{lemma}
Let \(\delta \in (0, 1)\). If \(B \in \real^{N \times n}\) is a matrix that satisfies:
\begin{equation*}
\|B^{T}B - I\|_{2} \leq \delta^{2}
\end{equation*}
Then \((1 - \delta) \leq \sigma_{\min}(B) \leq \sigma_{\max}(B) \leq (1 + \delta)\)

Conversely, if \(B\) satisfies:
\begin{equation*}
(1 - \delta) \leq \sigma_{\min}(B) \leq \sigma_{\max}(B) \leq (1 + \delta)
\end{equation*}
then \(\|B^{T}B - I\|_{2} \leq 3\delta\).
\end{lemma}

\begin{proof}
First note that:
\begin{equation*}
\|B^{T}B - I\|_{2} \leq \alpha \Leftrightarrow |\|Bx\|_{2}^{2} - 1| \leq \alpha
\end{equation*}
for any \(x \in S^{n - 1}\).
This can be proven as follows:
\begin{equation*}
\|B^{T}B - I\|_{2} \leq \alpha \Leftrightarrow \max_{x \in S^{n-1}} |x^{T}(B^{T}B - I)x| \leq \alpha \Leftrightarrow \max_{x \in S^{n-1}} |\|Bx\|_{2}^{2} - 1| \leq \alpha \Leftrightarrow |\|Bx\|_{2}^{2} - 1| \leq \alpha ~\forall x \in S^{n-1}
\end{equation*}

This leads to:
\begin{equation*}
1 - \sqrt{\alpha} \leq \sqrt{1 - \alpha} \leq \|Bx\|_{2} \leq \sqrt{1 + \alpha} \leq 1 + \sqrt{\alpha}
\end{equation*}

Now substitute \(\alpha = \delta^{2}\) to complete the proof for the first statement.

For the second statement, we know that \(\sigma_{\min}(B) = \sqrt{\lambda_{\min}(B^{T}B)}\) and \(\sigma_{\max}(B) = \sqrt{\lambda_{\max}(B^{T}B)}\). Therefore, the eigenvalues of \(B^{T}B\) lie in the interval \([(1 - \delta)^{2}, (1 + \delta)^{2}]\). This means that the maximum absolute eigenvalue of \(B^{T}B - I\) must be bounded as \(\max\{|(1 - \delta)^{2} - 1|, |(1 + \delta)^{2} - 1|\} \leq \max\{2\delta, 3\delta\} = 3\delta\), and this completes the proof.
\end{proof}

\subsubsection{Auxiliary lemmata}
\begin{lemma}
\label{lem:quadratic-random-mat-iid-entry}
Let \(A\) be a matrix whose entries are independent standard normal random variables. Then for any \(u, u' \in \real^{n}\) and \(v, v' \in \real^{N}\), we have:
\begin{equation*}
\Exp\left[(v^{T}Au - v'^{T}Au')^{2}\right] = \sum_{i=1}^{N}\sum_{j=1}^{n}(v_{i}u_{j} - v'_{i}u'_{j})^{2}
\end{equation*}
\end{lemma}
\begin{proof}
\begin{equation*}
v^{T}Au - v'^{T}Au' = \sum_{i=1}^{N}\sum_{j=1}^{n} v_{i}u_{j}A_{ij} - v'_{i}u'_{j}A_{ij} = \sum_{i=1}^{N}\sum_{j=1}^{n}A_{ij}\Delta_{ij}
\end{equation*}
for \(\Delta_{ij} = v_{i}u_{j} - v'_{i}u'_{j}\).

This implies:
\begin{align*}
(v^{T}Au - v'^{T}Au')^{2} &= \left(\sum_{i=1}^{N}\sum_{j=1}^{n}A_{ij}\Delta_{ij}\right)^{2} \\
&= \sum_{i=1}^{N}\sum_{j=1}^{n}\sum_{k=1}^{N}\sum_{l=1}^{n} A_{ij}\Delta_{ij}A_{kl}\Delta_{kl} \\
&= \sum_{i=1}^{N}\sum_{j=1}^{n} A_{ij}^{2}\Delta_{ij}^{2} + \sum_{i=1}^{N}\sum_{j=1}^{n}\sum_{\substack{k=1 \\ k \neq i}}^{N} \sum_{\substack{l=1 \\ l \neq j}}^{n} A_{ij}A_{kl}\Delta_{ij}\Delta_{kl} \\
\Rightarrow \Exp\left[(v^{T}Au - v'^{T}Au')^{2}\right] &= \sum_{i=1}^{N}\sum_{j=1}^{n} \Exp\left[A_{ij}^{2}\right]\Delta_{ij}^{2} + \sum_{i=1}^{N}\sum_{j=1}^{n}\sum_{\substack{k=1 \\ k \neq i}}^{N} \sum_{\substack{l=1 \\ l \neq j}}^{n} \underbrace{\Exp\left[A_{ij}A_{kl}\right]}_{=~0}\Delta_{ij}\Delta_{kl} \\
&= \sum_{i=1}^{N}\sum_{j=1}^{n} \Delta_{ij}^{2} = \sum_{i=1}^{N}\sum_{j=1}^{n}(v_{i}u_{j} - v'_{i}u'_{j})^{2}
\end{align*}
\end{proof}

\begin{lemma}
\label{lem:inner-to-norm}
For any \(u, u' \in S^{n-1}\) and \(v, v' \in S^{N-1}\), we have:
\begin{equation*}
\sum_{i=1}^{N}\sum_{j=1}^{n}(v_{i}u_{j} - v'_{i}u'_{j})^{2} \leq \|u - u'\|_{2}^{2} + \|v - v'\|_{2}^{2}
\end{equation*}
\end{lemma}
\begin{proof}
%\begin{align*}
%\sum_{i=1}^{N}\sum_{j=1}^{n}(v_{i}u_{j} - v'_{i}u'_{j})^{2} &= \sum_{i=1}^{N}\sum_{j=1}^{n}v_{i}^{2}u_{j}^{2} + {v'_{i}}^{2}{u'_{j}}^{2} - 2v_{i}v'_{i}u_{j}u'_{j} \\
%&= 2 - 2\sum_{i=1}^{N}\sum_{j=1}^{n}v_{i}v'_{i}u_{j}u'_{j} \\
%&\leq 4 - 2\sum_{i=1}^{N}\sum_{j=1}^{n}v_{i}v'_{i}u_{j}u'_{j} \\
%&= \sum_{i=1}^{N}v_{i}^{2} + \sum_{i=1}^{N}{v'_{i}}^{2} + \sum_{j=1}^{n}u_{j}^{2} + \sum_{j=1}^{n}{u'_{j}}^{2} - 2\sum_{i=1}^{N}\sum_{j=1}^{n}v_{i}v'_{i}u_{j}u'_{j} \\
%&\leq \sum_{i=1}^{N}v_{i}^{2} + {v'_{i}}^{2} - 2v_{i}v'_{i} + \sum_{j=1}^{n}u_{j}^{2} + {u'_{j}}^{2} - 2u_{j}u'_{j}
%\end{align*}
\end{proof}

\begin{lemma}
\label{lem:linear-random-vec}
Let \(g, h\) be random normal vectors with zero mean. Define \(Y_{u, v} = \inner{g}{u} + \inner{h}{v}\). Then for any \(u, u' \in \real^{n}\) and \(v, v' \in \real^{N}\), we have:
\begin{equation*}
\Exp\left[(Y_{u, v} - Y_{u', v'})^{2}\right] = \|u - u'\|_{2}^{2} + \|v - v'\|_{2}^{2}
\end{equation*}
\end{lemma}
\begin{proof}
\begin{gather*}
Y_{u, v} - Y_{u', v'} = \inner{g}{u - u'} + \inner{h}{v - v'} \\
\Rightarrow (Y_{u, v} - Y_{u', v'})^{2} = (\inner{g}{u - u'})^{2} + (\inner{h}{v - v'})^{2} + 2\inner{g}{u - u'}\inner{h}{v - v'} \\
\Rightarrow \Exp\left[(Y_{u, v} - Y_{u', v'})^{2}\right] = \underbrace{\Exp\left[(\inner{g}{u - u'})^{2}\right]}_{\Var(\calN(0, \|u - u'\|_{2}^{2}))} + \underbrace{\Exp\left[(\inner{h}{v - v'})^{2}\right]}_{\Var(\calN(0, \|v - v'\|_{2}))} + \underbrace{2\Exp\left[\inner{g}{u - u'}\inner{h}{v - v'}\right]}_{=~0}
\end{gather*}
\end{proof}
\end{document}
